# ğŸ’» web-crawling-extra

Este Ã© um projeto realizado para o processo seletivo da vaga de estÃ¡gio em web crawling, oferecido pela empresa @Birdie. O projeto tinha como desafio a coleta de informaÃ§Ãµes (*idSku, Nome do produto e Url*) de trÃªs categorias de produtos do site www.extra.com.br:

* ğŸ–¨ï¸ Impressoras;
* ğŸ“º Televisores;
* ğŸ§Š Geladeiras.

Para fazer a coleta dessas informaÃ§Ãµes foi usado o framework **Scrapy**, da linguagem **Python**. Utilizando esse framework, optei por criar trÃªs spiders, 1 para cada categoria. Com elas, foi possÃ­vel colher informaÃ§Ãµes de quase todos os produtos, localizados em uma API do extra - onde estavam as informaÃ§Ãµes como *nome, marca, url* - e do ViaVarejo - onde estavam o *idSku do produto, preÃ§o, parcelamento e o id do lojista*.

## Criando novo projeto com o framework Scrapy

1. Abra o seu terminal e instale o Scrapy:

   ```$ pip install scrapy```

2. Com o Scrapy instalado, crie um novo projeto atravÃ©s do comando:

    ```$ scrapy startproject nomeDoProjeto```
 

## ğŸ‘©â€ğŸ’» Processo de Desenvolvimento do CÃ³digo

Durante a anÃ¡lise da pÃ¡gina, que pode ser considerada bem dinÃ¢mica, por conta do uso de Javascript em diversos elementos da pÃ¡gina, localizei 2 responses interessantes ao clicar no botÃ£o "ver mais produtos":

![tempsnip](https://user-images.githubusercontent.com/82274021/126075819-0fa7a689-c990-4df2-a2e8-8db9a6206d81.jpg)






