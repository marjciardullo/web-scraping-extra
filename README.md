# üíª web-crawling-extra

Este √© um projeto realizado para o processo seletivo da vaga de est√°gio em web crawling, oferecido pela empresa @Birdie. O projeto tinha como desafio a coleta de informa√ß√µes (*idSku, Nome do produto e Url*) de tr√™s categorias de produtos do site www.extra.com.br:

* üñ®Ô∏è Impressoras;
* üì∫ Televisores;
* üßä Geladeiras.

Para fazer a coleta dessas informa√ß√µes foi usado o framework **Scrapy**, da linguagem **Python**. Utilizando esse framework, optei por criar tr√™s spiders, 1 para cada categoria. Com elas, foi poss√≠vel colher informa√ß√µes de quase todos os produtos, localizados em uma API do extra - onde estavam as informa√ß√µes como *nome, marca, url* - e do ViaVarejo - onde estavam o *idSku do produto, pre√ßo, parcelamento e o id do lojista*.

## üìÅ Criando novo projeto com o framework Scrapy

1. Abra o seu terminal e instale o Scrapy:

   ```$ pip install scrapy```

2. Com o Scrapy instalado, crie um novo projeto atrav√©s do comando:

    ```$ scrapy startproject nomeDoProjeto```
    
 
### üßÆ Importando m√≥dulos necess√°rios para o projeto

Dentro da p√°gina do seu projeto, importe os seguintes m√≥dulos:

1. M√≥dulo Json:

   ```import json```

2. M√≥dulo Math:
   
   ```import math```

## üë©‚Äçüíª Processo de Desenvolvimento do C√≥digo

Durante a an√°lise da p√°gina, que pode ser considerada bem din√¢mica, por conta do uso de Javascript em diversos elementos da p√°gina, localizei 2 responses interessantes ao clicar no bot√£o "ver mais produtos":

![tempsnip](https://user-images.githubusercontent.com/82274021/126075819-0fa7a689-c990-4df2-a2e8-8db9a6206d81.jpg)

Na segunda url, pude perceber que havia a altera√ß√£o de uma pequena informa√ß√£o conforme eu fui analisando a pagina√ß√£o desse link oferecido:

![tempsnipasssas](https://user-images.githubusercontent.com/82274021/126076573-ee1efca9-a18b-455f-b3b1-c24b74349100.jpg)

Diante dessa informa√ß√£o, constatei que, em cada p√°gina, haviam dados de apenas 20 produtos e o n√∫mero total de produtos daquela categoria espec√≠fica. Portanto, com essas informa√ß√µes valiosas, coletei o n√∫mero total e dividi pelo n√∫mero de produtos por p√°gina, o que resultou em um n√∫mero total de p√°ginas que eu deveria realizar um crawl.

   * *Para fazer um arredondamento do resultado dessa divis√£o, √© recomend√°vel utilizar a biblioteca math.*

Dentro dessa url, e ap√≥s a coleta da maior parte de informa√ß√µes sugeridas, percebi que, para coletar as informa√ß√µes restantes (localizadas na 1¬™ response na primeira imagem acima), eu precisava apenas coletar o Id dos produtos da p√°gina e junt√°-los √† nova Url:

![asjduasd](https://user-images.githubusercontent.com/82274021/126077162-bc56ee8e-0efd-4995-9ef7-5d27d08adcb4.jpg)

Ap√≥s a solu√ß√£o elaborada em rela√ß√£o √† pagina√ß√£o, coletei a maior parte de informa√ß√µes poss√≠veis e salvei em csv atrav√©s do comando:

   ```scrapy crawl nomeDaSpider -o nome-Do-Arquivo.csv```










